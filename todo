Symptom: for all images, layer 2 activates the exact same neurons every time. 
DONE
- start server devbox
- put random static in, see what comes out for layer 2 and layer 20
- do analysis on one that is more in the middle
- put an image in, see what comes out for both layers
- see what "read image" does
- read hugofry's code, does it calculate MSE before or after preprocessing? (not relevant, MSE is from activations so way after processing anyway)


1.
- established: the way images are passed in has a huge impact on the latents which come out. This means we should probably re-train
- check what exact format the tensors coming out of streaming tensor dataset come out in

2. 
- check what our prior training run was doing "load_image" strategy

3.
- test if to_tensor has any impact --> it does. Instead we want to use frombuffer to load the raw bytes then decode_jpeg

4. 
- update tardataset so it spits out correct tensors


5. 
- test multi worker loading of dataset
- start generating alternate dataset of activations
- sonja joseph's vit primsa isn't really related to SAEs
- repopulate queue


6. 
- make tardataset compatible with collate fn

7. 

8. 
- read imagenet curve detector https://arxiv.org/pdf/2406.03662
- get dataloader working without tensors
- test new image based tardataset

TODO NOW
- start generating alternate dataset of activations

    PREFLIGHT
    - re build docker image - done 
    - test docker image - done
    - push docker image - done
    - re-build queue - done
    - check run name
    - delete old s3 activations cache


TODO GENERAL
- look at position MSE
- read what people usually do with vision SAEs
- do a test run of retraining, compare to existing, see if it's getting better results
- calculate the mean feature latent across the image and use this for classification
- clean up that bullshit on s3